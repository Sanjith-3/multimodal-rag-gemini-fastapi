# -*- coding: utf-8 -*-
"""Copy of Multimodal_RAG

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J3KiSBd4nYVeFxtL9zqGr5x70XWDVn3r
"""

# install OCR dependencies for unstructured
!sudo apt-get install tesseract-ocr
!sudo apt-get install poppler-utils

!pip install htmltabletomd

# First, ensure we have a writable directory for NLTK data
import os

# Create a specific directory for NLTK data in Colab
nltk_data_dir = "/content/nltk_data"
if not os.path.exists(nltk_data_dir):
    os.makedirs(nltk_data_dir)

# Set the NLTK_DATA environment variable
os.environ['NLTK_DATA'] = nltk_data_dir

# Install NLTK and required packages
!pip install --upgrade nltk
!pip install --upgrade "unstructured[pdf]"

# Force download ALL potentially needed NLTK data to our specific directory
import nltk

resources_to_download = [
    'punkt',
    'averaged_perceptron_tagger',
    'maxent_ne_chunker',
    'words',
    'punkt_tab',
    'averaged_perceptron_tagger_eng'  # Adding the missing resource
]

for resource in resources_to_download:
    try:
        nltk.download(resource, download_dir=nltk_data_dir, quiet=True)
        print(f"Downloaded {resource} successfully")
    except Exception as e:
        print(f"Error downloading {resource}: {str(e)}")

# Clear any existing paths and add only our custom path
nltk.data.path = [nltk_data_dir]

# Additional step: Download the English model specifically
import urllib.request
import zipfile
import ssl

def download_eng_tagger():
    try:
        # Create directories if they don't exist
        tagger_dir = os.path.join(nltk_data_dir, 'taggers')
        if not os.path.exists(tagger_dir):
            os.makedirs(tagger_dir)

        # Download and extract the tagger
        url = "https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/taggers/averaged_perceptron_tagger.zip"

        # Handle SSL context
        context = ssl._create_unverified_context()

        print("Downloading English tagger...")
        filename, _ = urllib.request.urlretrieve(url, "tagger.zip", context=context)

        with zipfile.ZipFile(filename, 'r') as zip_ref:
            zip_ref.extractall(tagger_dir)

        os.rename(os.path.join(tagger_dir, "averaged_perceptron_tagger"),
                 os.path.join(tagger_dir, "averaged_perceptron_tagger_eng"))

        print("English tagger downloaded and installed successfully")

    except Exception as e:
        print(f"Error downloading English tagger: {str(e)}")
    finally:
        if os.path.exists("tagger.zip"):
            os.remove("tagger.zip")

# Download the English tagger
download_eng_tagger()

# Verify installation
def verify_nltk_resources():
    required_resources = [
        'tokenizers/punkt',
        'tokenizers/punkt_tab/english',
        'taggers/averaged_perceptron_tagger',
        'taggers/averaged_perceptron_tagger_eng',
        'chunkers/maxent_ne_chunker',
        'corpora/words'
    ]

    all_available = True
    for resource in required_resources:
        try:
            nltk.data.find(resource)
            print(f"✓ {resource} is available")
        except LookupError:
            print(f"✗ {resource} is NOT available")
            all_available = False

    return all_available

print("\nVerifying NLTK resources...")
resources_available = verify_nltk_resources()

if resources_available:
    print("\nAll resources are available! You can now try loading your PDF.")
else:
    print("\nSome resources are still missing. Please check the output above.")

!pip install langchain
!pip install langchain-openai
!pip install langchain-chroma
!pip install langchain-community
!pip install langchain-experimental

!pip install --upgrade tokenizers==0.21.0

from langchain_community.document_loaders import UnstructuredPDFLoader

doc = '/content/TATA_RR_01112022_Retail-02-November-2022-604637100.pdf'

# takes 1 min on Colab
loader = UnstructuredPDFLoader(file_path=doc,
                               strategy='hi_res',
                               extract_images_in_pdf=True,
                               infer_table_structure=True,
                               chunking_strategy="by_title", # section-based chunking
                               max_characters=4000, # max size of chunks
                               new_after_n_chars=4000, # preferred size of chunks
                               combine_text_under_n_chars=2000, # smaller chunks < 2000 chars will be combined into a larger chunk
                               mode='elements',
                               image_output_dir_path='./figures')
data = loader.load()

len(data)

[doc.metadata['category'] for doc in data]

print(data[2].page_content)

data[2].metadata['text_as_html']

from IPython.display import HTML, display, Markdown

display(Markdown(data[2].metadata['text_as_html']))

docs = []
tables = []

# Loop through all documents
for doc in data:
    # Check if 'text_as_html' exists in the document metadata
    if 'text_as_html' in doc.metadata:
        # Check if the content seems to be a table (based on the 'text_as_html' property)
        if 'table' in doc.metadata['text_as_html'].lower():
            doc.metadata['category'] = 'Table'  # Reclassify as 'Table'
            tables.append(doc)
        else:
            docs.append(doc)
    else:
        docs.append(doc)

# Check the lengths after reclassification
print(len(docs), len(tables))

for table in tables:
    print(table.page_content)
    print()

!pip install langchain-google-genai pillow tqdm



os.environ['GOOGLE_API_KEY'] = 'AIzaSyDyyEdkyuSwlePm04qkN1Qy52tOSEEuYY0'

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.schema.messages import HumanMessage
from langchain.prompts import ChatPromptTemplate
from typing import List, Tuple, Optional
import base64
import os
from PIL import Image
import io
from tqdm import tqdm

class GeminiUnifiedSummarizer:
    def __init__(self, api_key: str):
        self.llm = ChatGoogleGenerativeAI(
            model="gemini-1.5-pro",
            google_api_key=api_key,
            temperature=0.3,
            convert_system_message_to_human=True
        )

        # Define prompts
        self.text_prompt = """Summarize the following text in a concise and informative way.
        Focus on key points and maintain factual accuracy:

        {text}
        """

        self.table_prompt = """This is a table from a document. Describe its contents and key insights:

        {text}
        """

        self.image_prompt = """Analyze this image and provide a detailed description of its contents,
        focusing on any charts, graphs, or visual data if present.
        """

    def encode_image(self, image_path: str) -> Optional[str]:
        """Convert image to base64 string"""
        try:
            with open(image_path, "rb") as image_file:
                return base64.b64encode(image_file.read()).decode("utf-8")
        except Exception as e:
            print(f"Error encoding image {image_path}: {str(e)}")
            return None

    def process_text(self, text: str, is_table: bool = False) -> str:
        """Process text content using Gemini"""
        try:
            prompt = self.table_prompt if is_table else self.text_prompt
            messages = [HumanMessage(content=prompt.format(text=text))]
            response = self.llm.invoke(messages)
            return response.content
        except Exception as e:
            print(f"Error processing text: {str(e)}")
            return "Error generating summary"

    def process_image(self, image_path: str) -> str:
        """Process image content using Gemini"""
        try:
            # Prepare image data
            base64_image = self.encode_image(image_path)
            if not base64_image:
                return "Error: Could not encode image"

            # Create message with image
            message = HumanMessage(
                content=[
                    {
                        "type": "text",
                        "text": self.image_prompt
                    },
                    {
                        "type": "image_url",
                        "image_url": f"data:image/jpeg;base64,{base64_image}"
                    }
                ]
            )

            # Get response
            response = self.llm.invoke([message])
            return response.content
        except Exception as e:
            print(f"Error processing image: {str(e)}")
            return "Error generating image description"

    def summarize_documents(self, docs: List, tables: List, image_dir: str) -> Tuple[List[str], List[str], List[str]]:
        """Main function to summarize all content types"""
        text_summaries = []
        table_summaries = []
        image_summaries = []

        # Process text documents
        print(f"Processing {len(docs)} text documents...")
        for doc in tqdm(docs, desc="Processing text"):
            if hasattr(doc, 'page_content'):
                summary = self.process_text(doc.page_content)
                if summary != "Error generating summary":
                    text_summaries.append(summary)

        # Process tables
        print(f"\nProcessing {len(tables)} tables...")
        for table in tqdm(tables, desc="Processing tables"):
            if hasattr(table, 'page_content'):
                summary = self.process_text(table.page_content, is_table=True)
                if summary != "Error generating summary":
                    table_summaries.append(summary)

        # Process images
        if os.path.exists(image_dir):
            image_files = sorted([f for f in os.listdir(image_dir) if f.endswith('.jpg')])
            print(f"\nProcessing {len(image_files)} images...")
            for img_file in tqdm(image_files, desc="Processing images"):
                img_path = os.path.join(image_dir, img_file)
                summary = self.process_image(img_path)
                if "Error" not in summary:
                    image_summaries.append(summary)

        # Print statistics
        print(f"\nSuccessfully generated:")
        print(f"- {len(text_summaries)} text summaries")
        print(f"- {len(table_summaries)} table summaries")
        print(f"- {len(image_summaries)} image summaries")

        return text_summaries, table_summaries, image_summaries

# Example usage
def main():
    # Initialize summarizer with your API key
    api_key = os.getenv('GOOGLE_API_KEY')
    summarizer = GeminiUnifiedSummarizer(api_key)

    # Use your existing docs and tables from the UnstructuredPDFLoader
    text_summaries, table_summaries, image_summaries = summarizer.summarize_documents(
        docs=docs,
        tables=tables,
        image_dir='./figures'
    )

    # Print example summaries
    if text_summaries:
        print("\nExample text summary:", text_summaries[0])
    if table_summaries:
        print("\nExample table summary:", table_summaries[0])
    if image_summaries:
        print("\nExample image summary:", image_summaries[0])

if __name__ == "__main__":
    main()

import pandas as pd
import os
from IPython.display import HTML, display
import base64
from typing import List, Optional

class SummaryVisualizer:
    def __init__(self, image_dir: str = './figures'):
        self.image_dir = image_dir

    def encode_image(self, image_path: str) -> Optional[str]:
        """Convert image to base64 string"""
        try:
            with open(image_path, "rb") as image_file:
                return base64.b64encode(image_file.read()).decode("utf-8")
        except Exception as e:
            print(f"Error encoding image {image_path}: {str(e)}")
            return None

    def create_visualization(self, summaries: List[str], max_width: int = 200) -> pd.DataFrame:
        """Create and display a table of images and their summaries"""

        # Get list of image files
        image_files = sorted([f for f in os.listdir(self.image_dir) if f.endswith('.jpg')])

        # Prepare data
        data = []
        for idx, (img_file, summary) in enumerate(zip(image_files[:len(summaries)], summaries)):
            img_path = os.path.join(self.image_dir, img_file)
            base64_str = self.encode_image(img_path)
            if base64_str:
                data.append({
                    'Index': idx + 1,
                    'Image_Name': img_file,
                    'Summary': summary,
                    'Base64': base64_str
                })

        # Create DataFrame
        df = pd.DataFrame(data)

        # Create HTML table
        def create_html_table():
            html_content = '''
            <table style="border-collapse: collapse; width: 100%; margin: 20px 0;">
                <thead>
                    <tr style="background-color: #f8f9fa;">
                        <th style="border: 1px solid #dee2e6; padding: 12px; text-align: left;">No.</th>
                        <th style="border: 1px solid #dee2e6; padding: 12px; text-align: left;">Image</th>
                        <th style="border: 1px solid #dee2e6; padding: 12px; text-align: left;">File Name</th>
                        <th style="border: 1px solid #dee2e6; padding: 12px; text-align: left;">Gemini Summary</th>
                    </tr>
                </thead>
                <tbody>
            '''

            for _, row in df.iterrows():
                html_content += f'''
                    <tr>
                        <td style="border: 1px solid #dee2e6; padding: 12px;">{row['Index']}</td>
                        <td style="border: 1px solid #dee2e6; padding: 12px;">
                            <img src="data:image/jpeg;base64,{row['Base64']}"
                                 style="max-width: {max_width}px; height: auto;"
                                 alt="{row['Image_Name']}"/>
                        </td>
                        <td style="border: 1px solid #dee2e6; padding: 12px;">{row['Image_Name']}</td>
                        <td style="border: 1px solid #dee2e6; padding: 12px;">{row['Summary']}</td>
                    </tr>
                '''

            html_content += '''
                </tbody>
            </table>
            '''
            return html_content

        # Save CSV (without base64 images)
        csv_df = df[['Index', 'Image_Name', 'Summary']].copy()
        csv_df.to_csv('gemini_image_summaries.csv', index=False)
        print("CSV file saved as 'gemini_image_summaries.csv'")

        # Display HTML table
        display(HTML(create_html_table()))

        return df

# Example usage with the GeminiUnifiedSummarizer
def visualize_summaries(api_key: str):
    # Initialize summarizer
    summarizer = GeminiUnifiedSummarizer(api_key)

    # Get summaries
    text_summaries, table_summaries, image_summaries = summarizer.summarize_documents(
        docs=docs,
        tables=tables,
        image_dir='./figures'
    )

    # Initialize visualizer
    visualizer = SummaryVisualizer(image_dir='./figures')

    # Create visualization
    df = visualizer.create_visualization(image_summaries)

    return df

# Run visualization
if __name__ == "__main__":
    api_key = os.getenv('GOOGLE_API_KEY')
    df = visualize_summaries(api_key)















































from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch
import numpy as np

# Load model and tokenizer
tokenizer = AutoTokenizer.from_pretrained("google/pegasus-xsum")
model = AutoModelForSeq2SeqLM.from_pretrained("google/pegasus-xsum")

class PegasusSummarizer:
    def __init__(self, model, tokenizer, max_input_length=512, max_output_length=128, min_output_length=30):
        self.model = model
        self.tokenizer = tokenizer
        self.max_input_length = max_input_length
        self.max_output_length = max_output_length
        self.min_output_length = min_output_length

    def clean_text(self, text):
        """Clean and validate input text"""
        if not isinstance(text, str):
            text = str(text)

        # Remove excessive whitespace
        text = ' '.join(text.split())

        # Ensure minimum length
        if len(text.strip()) < 10:  # Arbitrary minimum length
            return None

        return text

    def __call__(self, prompt_dict):
        try:
            # Extract and clean text
            if isinstance(prompt_dict, dict) and 'element' in prompt_dict:
                text = self.clean_text(prompt_dict['element'])
            else:
                text = self.clean_text(prompt_dict)

            # Validate text
            if text is None or not text.strip():
                print("Invalid or empty text input")
                return "Error: Invalid input text"

            # Tokenize with proper truncation
            inputs = self.tokenizer(
                text,
                max_length=self.max_input_length,
                truncation=True,
                padding='max_length',
                return_tensors="pt"
            )

            # Generate summary
            with torch.no_grad():
                summary_ids = self.model.generate(
                    inputs["input_ids"],
                    max_length=self.max_output_length,
                    min_length=self.min_output_length,
                    num_beams=4,
                    length_penalty=2.0,
                    early_stopping=True,
                    no_repeat_ngram_size=3
                )

            # Decode summary
            summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)

            # Post-process summary
            if "\n" in text and "|" in text:  # Table detection
                summary = f"Table describing {summary.split('.')[0].lower()}. " + summary

            return summary

        except Exception as e:
            print(f"Error processing text: {str(e)}\nText preview: {text[:100]}...")
            return "Error generating summary"

def process_in_batches(documents, batch_size=5):
    """Process documents in batches with better error handling"""
    summaries = []

    # Filter out None or empty documents
    valid_docs = [doc for doc in documents if doc is not None and str(doc).strip()]

    for i in range(0, len(valid_docs), batch_size):
        batch = valid_docs[i:i + batch_size]
        try:
            batch_summaries = []
            for doc in batch:
                summary = pegasus({"element": doc})
                if summary != "Error generating summary":
                    batch_summaries.append(summary)
                else:
                    print(f"Skipping invalid document at index {i}")
            summaries.extend(batch_summaries)
        except Exception as e:
            print(f"Error processing batch {i//batch_size}: {str(e)}")
            continue
    return summaries

def clean_document(doc):
    """Clean and validate document content"""
    try:
        if hasattr(doc, 'page_content'):
            content = str(doc.page_content)
        else:
            content = str(doc)

        # Remove excessive whitespace
        content = ' '.join(content.split())

        # Skip if too short
        if len(content.strip()) < 10:
            return None

        return content
    except:
        return None

def summarize_documents(docs, tables):
    """Main function to summarize documents with input validation"""
    # Clean and validate documents
    text_docs = [clean_document(doc) for doc in docs]
    table_docs = [clean_document(table) for table in tables]

    # Remove None values
    text_docs = [doc for doc in text_docs if doc is not None]
    table_docs = [doc for doc in table_docs if doc is not None]

    print(f"Processing {len(text_docs)} valid text documents...")
    text_summaries = process_in_batches(text_docs)

    print(f"Processing {len(table_docs)} valid tables...")
    table_summaries = process_in_batches(table_docs)

    print(f"Successfully generated {len(text_summaries)} text summaries and {len(table_summaries)} table summaries")

    # Print some statistics
    if text_summaries:
        print(f"\nAverage text summary length: {np.mean([len(s) for s in text_summaries]):.1f} characters")
    if table_summaries:
        print(f"Average table summary length: {np.mean([len(s) for s in table_summaries]):.1f} characters")

    return text_summaries, table_summaries

# Initialize summarizer with conservative length limits
pegasus = PegasusSummarizer(
    model,
    tokenizer,
    max_input_length=512,  # Reduced from 1024 for stability
    max_output_length=128,
    min_output_length=30
)

# Example usage
text_summaries, table_summaries = summarize_documents(docs, tables)

# Print detailed examples if available
if text_summaries:
    print("\nFirst successful text summary:", text_summaries[0])
if table_summaries:
    print("\nFirst successful table summary:", table_summaries[0])

# Print example summaries
print("Text Summary Example:", text_summaries[3] if text_summaries else "No text summaries")
print("Table Summary Example:", table_summaries[3] if table_summaries else "No table summaries")

from transformers import AutoProcessor, AutoModelForImageTextToText
import base64
import os
from PIL import Image
import io
import torch
from tqdm import tqdm

# Load model and processor
processor = AutoProcessor.from_pretrained("Salesforce/blip-image-captioning-large")
model = AutoModelForImageTextToText.from_pretrained("Salesforce/blip-image-captioning-large")

def base64_to_pil(base64_string):
    """Convert base64 string to PIL Image"""
    try:
        image_data = base64.b64decode(base64_string)
        image = Image.open(io.BytesIO(image_data))
        return image
    except Exception as e:
        print(f"Error converting base64 to PIL: {str(e)}")
        return None

def encode_image(image_path):
    """Convert image to base64 string"""
    try:
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode("utf-8")
    except Exception as e:
        print(f"Error encoding image {image_path}: {str(e)}")
        return None

class BLIPSummarizer:
    def __init__(self, model, processor):
        self.model = model
        self.processor = processor
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model.to(self.device)

    def __call__(self, image, prompt=None):
        try:
            # Convert base64 to PIL if needed
            if isinstance(image, str):
                image = base64_to_pil(image)

            if image is None:
                return "Error: Invalid image"

            # Process image
            inputs = self.processor(images=image, return_tensors="pt").to(self.device)

            # Generate caption
            with torch.no_grad():
                generated_ids = self.model.generate(
                    pixel_values=inputs["pixel_values"],
                    max_length=50,
                    num_beams=5,
                    length_penalty=1.0
                )

            # Decode caption
            caption = self.processor.batch_decode(generated_ids, skip_special_tokens=True)[0]

            # Enhance caption for retrieval
            if "graph" in caption.lower() or "chart" in caption.lower():
                caption = f"Visual representation showing {caption}"
            elif "table" in caption.lower():
                caption = f"Tabular data displaying {caption}"

            return caption.strip()

        except Exception as e:
            print(f"Error generating caption: {str(e)}")
            return "Error generating caption"

def generate_img_summaries(path):
    """
    Generate summaries and base64 encoded strings for images
    path: Path to list of .jpg files
    """
    # Initialize lists
    img_base64_list = []
    image_summaries = []

    # Initialize summarizer
    summarizer = BLIPSummarizer(model, processor)

    # Get sorted list of jpg files
    image_files = sorted([f for f in os.listdir(path) if f.endswith('.jpg')])

    print(f"Found {len(image_files)} images to process")

    # Process each image with progress bar
    for img_file in tqdm(image_files, desc="Processing images"):
        try:
            img_path = os.path.join(path, img_file)

            # Get base64 encoding
            base64_image = encode_image(img_path)
            if base64_image:
                img_base64_list.append(base64_image)

                # Generate summary
                summary = summarizer(base64_image)
                image_summaries.append(summary)

        except Exception as e:
            print(f"Error processing {img_file}: {str(e)}")
            continue

    print(f"\nSuccessfully processed {len(image_summaries)} images")
    return img_base64_list, image_summaries

# Example usage
def main():
    IMG_PATH = './figures'

    # Create directory if it doesn't exist
    os.makedirs(IMG_PATH, exist_ok=True)


    # Generate summaries
    imgs_base64, image_summaries = generate_img_summaries(IMG_PATH)

    # Print results
    if image_summaries:
        print("\nExample summaries:")
        for i, summary in enumerate(image_summaries[:3]):
            print(f"Image {i+1}: {summary}")

    return imgs_base64, image_summaries

if __name__ == "__main__":
    imgs_base64, image_summaries = main()

import pandas as pd
import os
from IPython.display import HTML, display
import base64

def create_image_summary_table(imgs_base64, image_summaries, img_path):
    """Create a DataFrame with images and their summaries"""

    # Get list of image filenames
    image_files = sorted([f for f in os.listdir(img_path) if f.endswith('.jpg')])

    # Create DataFrame
    df = pd.DataFrame({
        'Image_Name': image_files[:len(image_summaries)],
        'Summary': image_summaries,
        'Base64': imgs_base64
    })

    # Function to create HTML img tag from base64
    def base64_to_html_img(base64_str, max_width=200):
        return f'<img src="data:image/jpeg;base64,{base64_str}" style="max-width: {max_width}px"/>'

    # Create HTML display version
    def create_html_table():
        html_content = '<table style="border-collapse: collapse; width: 100%;">'
        html_content += '''
            <tr style="background-color: #f2f2f2;">
                <th style="border: 1px solid #ddd; padding: 12px; text-align: left;">Image</th>
                <th style="border: 1px solid #ddd; padding: 12px; text-align: left;">File Name</th>
                <th style="border: 1px solid #ddd; padding: 12px; text-align: left;">Summary</th>
            </tr>
        '''

        for _, row in df.iterrows():
            html_content += f'''
                <tr>
                    <td style="border: 1px solid #ddd; padding: 12px;">{base64_to_html_img(row['Base64'])}</td>
                    <td style="border: 1px solid #ddd; padding: 12px;">{row['Image_Name']}</td>
                    <td style="border: 1px solid #ddd; padding: 12px;">{row['Summary']}</td>
                </tr>
            '''

        html_content += '</table>'
        return html_content

    # Save as CSV (without base64 images)
    csv_df = df[['Image_Name', 'Summary']].copy()
    csv_df.to_csv('image_summaries.csv', index=False)
    print("CSV file saved as 'image_summaries.csv'")

    # Display HTML table
    display(HTML(create_html_table()))

    return df

# Create and display the table
df = create_image_summary_table(imgs_base64, image_summaries, './figures')

!pip install datasets

from transformers import AutoModel, AutoTokenizer
import torch
from typing import List
from langchain_core.embeddings import Embeddings
import numpy as np

class NvidiaEmbeddings(Embeddings):
    def __init__(self, model_name="nvidia/NV-Embed-v2", max_length=512, device=None):
        # Initialize model and tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        self.model = AutoModel.from_pretrained(model_name, trust_remote_code=True)

        # Set device (GPU if available, else CPU)
        self.device = device if device else ('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = self.model.to(self.device)
        self.max_length = max_length

        # Put model in eval mode
        self.model.eval()

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings for a list of documents."""
        embeddings = []

        # Process in batches to avoid memory issues
        batch_size = 32
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]

            # Tokenize texts
            inputs = self.tokenizer(
                batch_texts,
                padding=True,
                truncation=True,
                max_length=self.max_length,
                return_tensors="pt"
            ).to(self.device)

            # Generate embeddings
            with torch.no_grad():
                outputs = self.model(**inputs)
                # Use [CLS] token embedding as the document embedding
                batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
                embeddings.extend(batch_embeddings)

        # Normalize embeddings
        normalized_embeddings = [
            embedding / np.linalg.norm(embedding)
            for embedding in embeddings
        ]

        return normalized_embeddings

    def embed_query(self, text: str) -> List[float]:
        """Generate embedding for a single query."""
        return self.embed_documents([text])[0]

# Modified code to use NVIDIA embeddings
# Initialize embeddings
nvidia_embeddings = NvidiaEmbeddings()

# Initialize Chroma with NVIDIA embeddings
chroma_db = Chroma(
    collection_name="mm_rag",
    embedding_function=nvidia_embeddings,
    collection_metadata={"hnsw:space": "cosine"},
)

# Initialize Redis store
client = get_client('redis://localhost:6379')
redis_store = RedisStore(client=client)

# Create retriever with NVIDIA embeddings
retriever_multi_vector = create_multi_vector_retriever(
    redis_store,
    chroma_db,
    text_summaries,
    text_docs,
    table_summaries,
    table_docs,
    image_summaries,
    imgs_base64,
)

# Function to test the embeddings
def test_embeddings(embeddings_model, texts: List[str]):
    print("Testing embeddings generation...")
    try:
        # Test document embeddings
        doc_embeddings = embeddings_model.embed_documents(texts[:2])
        print(f"Successfully generated embeddings for {len(doc_embeddings)} documents")
        print(f"Embedding dimension: {len(doc_embeddings[0])}")

        # Test query embedding
        query_embedding = embeddings_model.embed_query(texts[0])
        print(f"Successfully generated query embedding of dimension {len(query_embedding)}")

        return True
    except Exception as e:
        print(f"Error testing embeddings: {str(e)}")
        return False

# Test the embeddings if needed
# sample_texts = ["This is a test document", "Another test document"]
# test_embeddings(nvidia_embeddings, sample_texts)

